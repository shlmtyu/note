{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"intro_to_sparse_data_and_embeddings_ex4-1.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"0489H1boPtLI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":738},"outputId":"69cfb517-e3e5-444a-e1f0-f5f1a7e3ab2c","executionInfo":{"status":"ok","timestamp":1534733647694,"user_tz":-480,"elapsed":28539,"user":{"displayName":"joyce li","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"100989403678002079373"}}},"cell_type":"code","source":["from __future__ import print_function\n","\n","import collections\n","import io\n","import math\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from IPython import display\n","from sklearn import metrics\n","\n","tf.logging.set_verbosity(tf.logging.ERROR)\n","train_url = 'https://dl.google.com/mlcc/mledu-datasets/sparse-data-embedding/train.tfrecord'\n","train_path = tf.keras.utils.get_file(train_url.split('/')[-1], train_url)\n","test_url = 'https://dl.google.com/mlcc/mledu-datasets/sparse-data-embedding/test.tfrecord'\n","test_path = tf.keras.utils.get_file(test_url.split('/')[-1], test_url)\n","\n","def _parse_function(record):\n","  \"\"\"Extracts features and labels.\n","  \n","  Args:\n","    record: File path to a TFRecord file    \n","  Returns:\n","    A `tuple` `(labels, features)`:\n","      features: A dict of tensors representing the features\n","      labels: A tensor with the corresponding labels.\n","  \"\"\"\n","  features = {\n","    \"terms\": tf.VarLenFeature(dtype=tf.string), # terms are strings of varying lengths\n","    \"labels\": tf.FixedLenFeature(shape=[1], dtype=tf.float32) # labels are 0 or 1\n","  }\n","  \n","  parsed_features = tf.parse_single_example(record, features)\n","  \n","  terms = parsed_features['terms'].values\n","  labels = parsed_features['labels']\n","\n","  return  {'terms':terms}, labels\n","\n","# Create the Dataset object\n","ds = tf.data.TFRecordDataset(train_path)\n","# Map features and labels with the parse function\n","ds = ds.map(_parse_function)\n","\n","ds\n","\n","# Create an input_fn that parses the tf.Examples from the given files,\n","# and split them into features and targets.\n","def _input_fn(input_filenames, num_epochs=None, shuffle=True):\n","  \n","  # Same code as above; create a dataset and map features and labels\n","  ds = tf.data.TFRecordDataset(input_filenames)\n","  ds = ds.map(_parse_function)\n","\n","  if shuffle:\n","    ds = ds.shuffle(10000)\n","\n","  # Our feature data is variable-length, so we pad and batch\n","  # each field of the dataset structure to whatever size is necessary     \n","  ds = ds.padded_batch(25, ds.output_shapes)\n","  \n","  ds = ds.repeat(num_epochs)\n","\n","  \n","  # Return the next batch of data\n","  features, labels = ds.make_one_shot_iterator().get_next()\n","  return features, labels\n","\n","# 54 informative terms that compose our model vocabulary \n","informative_terms = (\"bad\", \"great\", \"best\", \"worst\", \"fun\", \"beautiful\",\n","                     \"excellent\", \"poor\", \"boring\", \"awful\", \"terrible\",\n","                     \"definitely\", \"perfect\", \"liked\", \"worse\", \"waste\",\n","                     \"entertaining\", \"loved\", \"unfortunately\", \"amazing\",\n","                     \"enjoyed\", \"favorite\", \"horrible\", \"brilliant\", \"highly\",\n","                     \"simple\", \"annoying\", \"today\", \"hilarious\", \"enjoyable\",\n","                     \"dull\", \"fantastic\", \"poorly\", \"fails\", \"disappointing\",\n","                     \"disappointment\", \"not\", \"him\", \"her\", \"good\", \"time\",\n","                     \"?\", \".\", \"!\", \"movie\", \"film\", \"action\", \"comedy\",\n","                     \"drama\", \"family\", \"man\", \"woman\", \"boy\", \"girl\")\n","\n","terms_feature_column = tf.feature_column.categorical_column_with_vocabulary_list(key=\"terms\", vocabulary_list=informative_terms)\n","\n","\n","\n","\n","\n","########################## SOLUTION CODE ########################################\n","terms_embedding_column = tf.feature_column.embedding_column(terms_feature_column, dimension=2)\n","feature_columns = [ terms_embedding_column ]\n","\n","my_optimizer = tf.train.AdagradOptimizer(learning_rate=0.1)\n","my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n","\n","classifier = tf.estimator.DNNClassifier(\n","  feature_columns=feature_columns,\n","  hidden_units=[10,10],\n","  optimizer=my_optimizer\n",")\n","#################################################################################\n","\n","classifier.train(\n","  input_fn=lambda: _input_fn([train_path]),\n","  steps=1000)\n","\n","evaluation_metrics = classifier.evaluate(\n","  input_fn=lambda: _input_fn([train_path]),\n","  steps=1000)\n","print(\"Training set metrics:\")\n","for m in evaluation_metrics:\n","  print(m, evaluation_metrics[m])\n","print(\"---\")\n","\n","evaluation_metrics = classifier.evaluate(\n","  input_fn=lambda: _input_fn([test_path]),\n","  steps=1000)\n","\n","print(\"Test set metrics:\")\n","for m in evaluation_metrics:\n","  print(m, evaluation_metrics[m])\n","print(\"---\")\n","\n","classifier.get_variable_names()"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Training set metrics:\n","accuracy 0.78744\n","accuracy_baseline 0.5\n","auc 0.8688475\n","auc_precision_recall 0.8579699\n","average_loss 0.45235384\n","label/mean 0.5\n","loss 11.3088455\n","precision 0.77314883\n","prediction/mean 0.50378036\n","recall 0.8136\n","global_step 1000\n","---\n","Test set metrics:\n","accuracy 0.7842\n","accuracy_baseline 0.5\n","auc 0.8668134\n","auc_precision_recall 0.8546963\n","average_loss 0.45561028\n","label/mean 0.5\n","loss 11.390257\n","precision 0.77220136\n","prediction/mean 0.50234956\n","recall 0.80624\n","global_step 1000\n","---\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["['dnn/hiddenlayer_0/bias',\n"," 'dnn/hiddenlayer_0/bias/t_0/Adagrad',\n"," 'dnn/hiddenlayer_0/kernel',\n"," 'dnn/hiddenlayer_0/kernel/t_0/Adagrad',\n"," 'dnn/hiddenlayer_1/bias',\n"," 'dnn/hiddenlayer_1/bias/t_0/Adagrad',\n"," 'dnn/hiddenlayer_1/kernel',\n"," 'dnn/hiddenlayer_1/kernel/t_0/Adagrad',\n"," 'dnn/input_from_feature_columns/input_layer/terms_embedding/embedding_weights',\n"," 'dnn/input_from_feature_columns/input_layer/terms_embedding/embedding_weights/t_0/Adagrad',\n"," 'dnn/logits/bias',\n"," 'dnn/logits/bias/t_0/Adagrad',\n"," 'dnn/logits/kernel',\n"," 'dnn/logits/kernel/t_0/Adagrad',\n"," 'global_step']"]},"metadata":{"tags":[]},"execution_count":1}]}]}